{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First Order Egograph Analysis: Twitter User @PietroMonticone\n",
    "\n",
    "In this notebook we analyze the second order egograph of a Twitter account with $\\sim10^3$ followees."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "# Enable interactive numpy and matplotlib\n",
    "%pylab inline\n",
    "\n",
    "# Data Wrangling \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Data Analysis\n",
    "import powerlaw as pwl\n",
    "\n",
    "# Data Visualization\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt \n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "# Network Analysis \n",
    "import networkx as nx\n",
    "from networkx.algorithms import community\n",
    "import networkx.algorithms.centrality as nc\n",
    "import social_physics as soc\n",
    "\n",
    "# Network Epidemiology \n",
    "import EoN\n",
    "\n",
    "# Data Visualization\n",
    "import seaborn as sns\n",
    "from netwulf import visualize\n",
    "\n",
    "# Other Utilities \n",
    "import sys, os, os.path\n",
    "import itertools\n",
    "from progressbar import ProgressBar, Bar, Percentage\n",
    "from operator import itemgetter\n",
    "from collections import Counter\n",
    "from collections import defaultdict\n",
    "import random\n",
    "\n",
    "# Reload Custom Modules\n",
    "from importlib import reload\n",
    "soc = reload(soc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graph Data Collection\n",
    "\n",
    "Load the graph as a edge list outputted by scraping libraries as `tweepy` or `rtweet`. \n",
    "\n",
    "A custom function `rtweet_to_networkx` has been written for this purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the csv files for the first and second order egonetwork data \n",
    "fo = \"/Users/pietromonticone/github/SocialPhysicsProject/Data/EdgeLists/pietro_friends.csv\"\n",
    "so =  \"/Users/pietromonticone/github/SocialPhysicsProject/Data/EdgeLists/pietro.csv\"\n",
    "\n",
    "# Convert rtweet output (.csv) to a networkx graph object\n",
    "G = soc.rtweet_to_networkx(fo, so)\n",
    "\n",
    "# Rename the graph \n",
    "G.name = \"Twitter 1st Order Followee EgoGraph\"\n",
    "\n",
    "# Show the basic attributes of the graph\n",
    "print(nx.info(G))\n",
    "\n",
    "# Relable the nodes (from strings of Twitter IDs to integers)\n",
    "G = nx.convert_node_labels_to_integers(G, first_label=0, ordering='default', label_attribute=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graph Data Visualization "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python \n",
    "visualize(G)\n",
    "```\n",
    "\n",
    "![](./Images/Twitter/@PietroMonticone1NW.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graph Data Analysis \n",
    "\n",
    "We proceed by visualizing the normalized degree distributions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Undirected Degree Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get undirected degree distribution \n",
    "undirected_degree_distribution, degree_mean, degree_variance = soc.get_degree_distribution(G, \"degree\")\n",
    "\n",
    "# Set figure size\n",
    "plt.figure(figsize=(7,4))\n",
    "\n",
    "# Plot undirected degree distribution \n",
    "soc.plot_degree_distribution(undirected_degree_distribution, \n",
    "                             title = \"Undirected Degree Distribution\", \n",
    "                             log = False, \n",
    "                             display_stats = True)\n",
    "\n",
    "# Show mean and variance of the undirected degree distribution \n",
    "print(\"Mean = \", degree_mean,\"\\nVar = \", degree_variance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In-Degree Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get in-degree distribution \n",
    "in_degree_distribution, indegree_mean, indegree_variance = soc.get_degree_distribution(G, \"in_degree\")\n",
    "\n",
    "# Set figure size\n",
    "plt.figure(figsize=(7,4))\n",
    "\n",
    "# Plot in-degree distribution \n",
    "soc.plot_degree_distribution(in_degree_distribution, \n",
    "                             title = \"In-Degree Distribution\",\n",
    "                             log_binning = None, \n",
    "                             display_stats = True)\n",
    "\n",
    "# Show mean and variance of the in-degree degree distribution \n",
    "print(\"Mean = \",indegree_mean,\"\\nVar = \", indegree_variance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Out-Degree Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get out-degree distribution \n",
    "out_degree_distribution, outdegree_mean, outdegree_variance = soc.get_degree_distribution(G, \"out_degree\")\n",
    "\n",
    "# Set figure size\n",
    "plt.figure(figsize=(7,4))\n",
    "\n",
    "# Plot out-degree distribution \n",
    "soc.plot_degree_distribution(out_degree_distribution, \n",
    "                             title = \"Out-Degree Distribution\",\n",
    "                             log_binning = None, \n",
    "                             display_stats = True)\n",
    "\n",
    "# Show mean and variance of the out-degree degree distribution \n",
    "print(\"Mean = \",outdegree_mean,\"\\nVar = \", outdegree_variance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected $\\langle k_{in}\\rangle = \\langle k_{out}\\rangle$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logarithmic Binning\n",
    "The black line is the empirical linearly binned pdf. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set figure size\n",
    "plt.figure(figsize=(7,4))\n",
    "\n",
    "# Plot\n",
    "pwl_distribution = soc.power_law_plot(graph = G, log = True, linear_binning = False, bins = 1000, draw = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The empirical PDF doesn't interpolate because it is obtained via linear binning while the red data points represent the logarithmic binning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Binning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set figure size\n",
    "plt.figure(figsize=(7,4))\n",
    "\n",
    "# Plot \n",
    "pwl_distribution = soc.power_law_plot(graph = G, log = True, linear_binning = True, bins = 1000, draw = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The figure above interpolates because it uses linear binnig both for scatter plot and pdf binning. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Power Law Fitting \n",
    "\n",
    "#### Parameters Estimation\n",
    "\n",
    "Here we estimate the measure to which the network follows a power law, and compare it with other common distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_function = pwl.Fit(list(undirected_degree_distribution.values()))\n",
    "\n",
    "print(\"Exponent = \", fit_function.power_law.alpha)\n",
    "print(\"Sigma (error associated to exponent) = \",fit_function.power_law.sigma)\n",
    "xmin = fit_function.power_law.xmin\n",
    "print(\"x_min = \",xmin)\n",
    "print(\"Kolmogorov-Smirnov distance = \",fit_function.power_law.D)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because the fitted $x_{min} = 224$, let's require it to be a little higher prior to fitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_function_fix_xmin = pwl.Fit(list(undirected_degree_distribution.values()),xmin= 25) \n",
    "print(\"Exponent = \", fit_function_fix_xmin.power_law.alpha)\n",
    "print(\"Sigma (error associated to exponent) = \",fit_function_fix_xmin.power_law.sigma)\n",
    "print(\"x_min = \",fit_function_fix_xmin.power_law.xmin)\n",
    "print(\"Kolmogorov-Smirnov distance = \",fit_function_fix_xmin.power_law.D)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the error (sigma) is way lower than before, but Kolmogorov-Smironv is higher as expected (because we fixed $x_{min}$ prior to fitting so we have lowered the degrees of freedom by one). Thus we confirmed that a power law fitting is good only near the tail because $x_{min} = 224$.<br>\n",
    "Let us now compare the actual PDF with the fitted power law near the tail.<br>\n",
    "<span style=\"color:blue\">**BLUE**</span> : Fitted power law. <br>\n",
    "<span style=\"color:black\">**BLACK**</span> : Plotted PDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set figure size\n",
    "plt.figure(figsize=(7,4))\n",
    "\n",
    "pwl_distribution = soc.power_law_plot(graph = G, log = True, linear_binning = False, bins = 90, draw = True, x_min = xmin)\n",
    "\n",
    "fit_function.power_law.plot_pdf(color='b', linestyle='-', linewidth=1)\n",
    "\n",
    "plt.xticks(fontsize=16)\n",
    "plt.yticks(fontsize=16)\n",
    "plt.xlabel('$k$', fontsize=19)\n",
    "plt.ylabel('$P(k)$', fontsize=19)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set figure size\n",
    "plt.figure(figsize=(7,4))\n",
    "\n",
    "#plt.plot(x,y,'ro')\n",
    "\n",
    "pwl_distribution = soc.power_law_plot(graph = G, log = True,linear_binning = False, bins = 90, draw = True, x_min = xmin)\n",
    "\n",
    "fit_function_fix_xmin.power_law.plot_pdf(color='b', linestyle='-', linewidth=1)\n",
    "\n",
    "#fig.legend(fontsize=22)\n",
    "plt.xticks(fontsize=16)\n",
    "plt.yticks(fontsize=16)\n",
    "plt.xlabel('$k$', fontsize=19)\n",
    "plt.ylabel('$P(k)$', fontsize=19)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now compare power law against other probability distributions. Remember that $R$ is the log-likelihood ratio between the two candidate distributions which will be positive if the data is more likely in the first distribution, and negative if the data is more likely in the second distribution. The significance value for that direction is $p$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "R,p = fit_function.distribution_compare('power_law', 'exponential', normalized_ratio=True) \n",
    "R,p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "R,p = fit_function.distribution_compare('power_law', 'lognormal_positive', normalized_ratio=True)\n",
    "R,p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "R,p = fit_function.distribution_compare('power_law', 'truncated_power_law', normalized_ratio=True)\n",
    "R,p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "R,p = fit_function.distribution_compare('power_law', 'stretched_exponential', normalized_ratio=True)\n",
    "R,p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us also compare with the truncated power law:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "R,p = fit_function_fix_xmin.distribution_compare('power_law', 'exponential', normalized_ratio=True) \n",
    "R,p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "R,p = fit_function_fix_xmin.distribution_compare('power_law', 'lognormal_positive', normalized_ratio=True) \n",
    "R,p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "R,p = fit_function_fix_xmin.distribution_compare('power_law', 'stretched_exponential', normalized_ratio=True) \n",
    "R,p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Centrality Metrics \n",
    "\n",
    "We compute and visualize the portfolio of centrality metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Degree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get degree centrality \n",
    "degree_centrality = soc.get_centrality(G, \"degree\")\n",
    "\n",
    "# Set figure size\n",
    "plt.figure(figsize=(7,4))\n",
    "\n",
    "# Plot centrality distribution\n",
    "soc.plot_centrality_distribution(G, degree_centrality, \"Blue\", 15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Closeness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get closeness centrality (computationally intensive!)\n",
    "closeness_centrality = soc.get_centrality(G, \"closeness\")\n",
    "\n",
    "# Set figure size\n",
    "plt.figure(figsize=(7,4))\n",
    "\n",
    "# Plot centrality distribution\n",
    "soc.plot_centrality_distribution(G, closeness_centrality, \"Blue\", 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Betweenness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get degree centrality \n",
    "betweenness_centrality = soc.get_centrality(G, \"betweenness\")\n",
    "\n",
    "# Set figure size\n",
    "plt.figure(figsize=(7,4))\n",
    "\n",
    "# Plot centrality distribution\n",
    "soc.plot_centrality_distribution(G, betweenness_centrality, \"Blue\", 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eigenvector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get degree centrality \n",
    "eigenvector_centrality = soc.get_centrality(G, \"eigenvector\")\n",
    "\n",
    "# Set figure size\n",
    "plt.figure(figsize=(7,4))\n",
    "\n",
    "x_centrality=[]\n",
    "y_centrality=[]\n",
    "     \n",
    "for i in eigenvector_centrality:\n",
    "    x_centrality.append(i[0])\n",
    "    y_centrality.append(i[1])\n",
    "\n",
    "plt.scatter(x_centrality,y_centrality, color=\"Blue\", marker=\"o\",alpha=0.50) \n",
    "#plt.yscale('log')\n",
    "#plt.xscale('log')\n",
    "plt.xlabel('$x$', fontsize = 15)\n",
    "plt.ylabel('$P(x)$', fontsize = 15)\n",
    "plt.xticks(fontsize=12)\n",
    "plt.yticks(fontsize=12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Katz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get degree centrality \n",
    "katz_centrality = soc.get_centrality(G, \"katz\")\n",
    "\n",
    "# Set figure size\n",
    "plt.figure(figsize=(7,4))\n",
    "\n",
    "# Plot centrality distribution\n",
    "soc.plot_centrality_distribution(G, katz_centrality, \"Blue\", 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PageRank\n",
    "$$x_i=(1-\\alpha) \\sum_{j}A^{T}_{ij}\\frac{x_j}{k^{out}_j}+\\frac{\\alpha}{N}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get degree centrality \n",
    "pagerank_centrality = soc.get_centrality(G, \"pagerank\")\n",
    "\n",
    "# Set figure size\n",
    "plt.figure(figsize=(7,4))\n",
    "\n",
    "# Plot centrality distribution\n",
    "soc.plot_centrality_distribution(G, pagerank_centrality, \"Blue\", 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connectivity \n",
    "\n",
    "Here we explore the connectivity of the graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the connectivity of the analyzed graph\n",
    "print(\"The graph has\", G.number_of_nodes(), \"nodes and\", G.number_of_edges(),\"edges.\")\n",
    "print(\"Is the (directed) graph weakly connected?\", nx.is_weakly_connected(G),\".\")  \n",
    "print(\"Is the (directed) graph strongly connected?\", nx.is_strongly_connected(G),\".\")\n",
    "G_weakly_cc = list(nx.weakly_connected_components(G))\n",
    "print(\"The graph has\", len(G_weakly_cc),\"weakly connected components.\")\n",
    "print(\"The sizes of the weakly connected components are\", [len(c) for c in sorted(G_weakly_cc, key=len, reverse=True)],\".\")\n",
    "G_strongly_cc = list(nx.strongly_connected_components(G))\n",
    "print(\"The graph has\", len(G_strongly_cc),\"strongly connected components.\")\n",
    "print(\"The sizes of the strongly connected components are\", [len(c) for c in sorted(G_strongly_cc, key=len, reverse=True)],\".\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering \n",
    "\n",
    "Here we compute the **average clustering coefficient** and the **global clustering coefficient**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Consider the undirected version (G -> U)\n",
    "U = G.to_undirected()\n",
    "\n",
    "# Rename the undirected graph \n",
    "U.name = \"Twitter Undirected EgoGraph\"\n",
    "\n",
    "# Show the basic attributes of U vs. G \n",
    "print(nx.info(U), \"\\n\")\n",
    "print(nx.info(G))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Global Clustering Coefficient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The global clustering coefficient measures the number of triangles in the network and it's defined as\n",
    "\n",
    "$$ C_\\Delta = \\frac{3 \\times \\text{triangles}}{\\text{triplets}} $$\n",
    "\n",
    "In order to compare our graph with theorical models (of the same size), it is thus sufficient to evaluate the number of triangles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the global clustering coefficient of U (the fraction of all possible triangles in the network)\n",
    "print(\"Global clustering coefficient = \", nx.transitivity(G))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Average Clustering Coefficient\n",
    "\n",
    "The overall level of clustering in a network is measured by Watts and Strogatz as the average of the local clustering coefficients of all the vertices $n$:\n",
    "\n",
    "$$\\bar{C} = \\frac{1}{n}\\sum_{i=1}^{n} C_i.$$\n",
    "\n",
    "\n",
    "It is worth noting that this metric places more weight on the low degree nodes, while the transitivity ratio places more weight on the high degree nodes. In fact, a weighted average where each local clustering score is weighted by $k_i(k_i-1)$ is identical to the global clustering coefficient. <br> As per [this](https://networkx.github.io/documentation/stable/reference/algorithms/generated/networkx.algorithms.cluster.clustering.html) and [this](https://networkx.github.io/documentation/stable/_modules/networkx/algorithms/cluster.html ) resources we notice that Networkx's `average_clustering` function automatically takes care of the network being directed or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G_avg_cc =  nx.average_clustering(G)\n",
    "print(\"The average clustering coefficient is \",G_avg_cc) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Path-ology  \n",
    "\n",
    "### Average Shortest Path Length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(\"The average shortest path length is \", nx.average_shortest_path_length(G),\"\") # Graph is not weakly connected.\n",
    "\n",
    "average_degree = sum(list(dict(G.degree()).values()))/len(G.degree())\n",
    "GWCC = list(G_weakly_cc[0])\n",
    "\n",
    "print(\"Since the graph is not weakly connected, but one of its 3 weakly connected compoments amounts for \",len(GWCC)/len(G), \"of the nodes count, we approximate its averaege shortest path length with that of its bigger weakly connected component, which is:\", nx.average_shortest_path_length(G.subgraph(GWCC)), \"\\nLet's compare it with lnlnN = \", math.log(math.log(len(GWCC))), \"(ultra small world)\\nand with lnN/lnlnN = \", math.log(len(GWCC))/math.log(math.log(len(GWCC))), \"(equivalent to a power law with exponent 3)\\nand with lnN = \", math.log(len(GWCC))/math.log(average_degree), \"equivalent to a random network.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparisons with Random Models \n",
    "\n",
    "### G vs. ER\n",
    "\n",
    "The most natural benchmark is a ER (random) network with the same number of nodes and links. In a ER network, the $p_k$ is Poissonian (an exponential decay), so let's compare G with random **Erdos-Renyi** graph with the same average connectivity and number of nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nnodes = U.number_of_nodes()\n",
    "nedges = U.number_of_edges()\n",
    "plink = 2*nedges/(nnodes*(nnodes-1)) # 2* because it is undirected\n",
    "\n",
    "ER = nx.fast_gnp_random_graph(nnodes, plink)\n",
    "\n",
    "average_degree = sum(list(dict(ER.degree()).values()))/len(ER.degree())\n",
    "\n",
    "# Connectivity\n",
    "print(\"The ER graph has\", len(ER), \"nodes\", \"and\",len(ER.edges()),\"edges.\\n The difference between its maximum and minimun degree is:\",max(list(dict(ER.degree).values()))-min(list(dict(ER.degree).values())),\", while the sane difference in our network is:\", max(list(dict(U.degree).values()))-min(list(dict(U.degree).values())),\"which is higher, confirming that real nertworks are not random.\")\n",
    "\n",
    "# Test connectedness\n",
    "print(\"Is the ER graph simply connected ?\", nx.is_connected(ER), \". Infact the average degree is:\",\n",
    "      average_degree,\"and the natural log of the number of nodes is\", \n",
    "      math.log(nnodes),\"which is smaller, then we are in the connected regime.\")\n",
    "\n",
    "# Average clustering coefficient\n",
    "print(\"The average clustering coefficient of ER is\", nx.average_clustering(ER),\"which, if compared with <k>/N\", average_degree/(nnodes), \"we can observe they are similar as expected. But it is approximately one order of magnitude less than the egonetwork's one \")\n",
    "\n",
    "print(\"The transitivity of the network is\", nx.transitivity(ER))\n",
    "\n",
    "# Average shortest path\n",
    "print(\"The ER graph is small world since the average shortest path is\", nx.average_shortest_path_length(ER),\"which we compare with lnN/ln(<k>):\" ,math.log(nnodes)/math.log(average_degree),\"to check the small world effect\", \"\\nand the expected result is lnN/ln(<k>):\", math.log(len(ER))/math.log(average_degree))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### G vs. AB\n",
    "\n",
    "Thinking about  a broad (not exponential decaying) distribution, more like a power law, we may think about a AB network (Albert-Barabasi), so let's compare G with random **Albert-Barabasi** graph with the same average connectivity and number of nodes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = U.number_of_nodes()\n",
    "m = int(U.number_of_edges() / n + 0.5) # 0.5 added to correctly round\n",
    "\n",
    "AB = nx.barabasi_albert_graph(n,m)\n",
    "\n",
    "# Test connectedness\n",
    "print(\"Is the AB graph simply connected ?\", nx.is_connected(AB))\n",
    "# Connectivity\n",
    "print(\"The AB graph has\", len(AB), \"nodes\", \"and\",len(AB.edges()),\"edges ..\\n The difference between its maximum and minimun degree is:\",max(list(dict(AB.degree).values()))-min(list(dict(AB.degree).values())), \", while the sane difference in our network is:\", max(list(dict(U.degree).values()))-min(list(dict(U.degree).values())), \"which is similar, confirming that albert barabasi captures the fundamental mechanisms that underly real network formation better than a random network would.\")\n",
    "\n",
    "# Average clustering coefficient\n",
    "print(\"The average clustering coefficient of AB is\", nx.average_clustering(AB),\". We may compare it with the predicted C_l = (m*ln(N)^2)/(4*N) = \",(m*(math.log(n))**2)/(4*n),\"while the global clustering coefficient is: \",nx.transitivity(AB),\".\")\n",
    "\n",
    "# Average shortest path\n",
    "print(\"The AB graph is small world since the average shortest path is\", nx.average_shortest_path_length(AB),\"and the expected result is lnN/lnlnN\", math.log(len(AB))/math.log(math.log(len(AB))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's verify that an AB network follows a power law distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the degree distribution\n",
    "AB_degree = dict(AB.degree()).values()\n",
    "AB_degree_distribution = Counter(AB_degree)\n",
    "\n",
    "# Plot the degree frequency distribution & \n",
    "# the probability density function \n",
    "plt.figure(figsize=(10,7))\n",
    "x=[]\n",
    "y=[]\n",
    "for i in sorted(AB_degree_distribution):   \n",
    "    x.append(i)\n",
    "    y.append(float(AB_degree_distribution[i])/len(AB))\n",
    "\n",
    "plt.plot(np.array(x),np.array(y))\n",
    "pwl.plot_pdf(list(AB_degree))\n",
    "\n",
    "plt.xlabel('$k$', fontsize=18)\n",
    "plt.ylabel('$P(k)$', fontsize=18)\n",
    "plt.xticks(fontsize=14)\n",
    "plt.yticks(fontsize=14)\n",
    "plt.yscale('log')\n",
    "plt.xscale('log')\n",
    "plt.axis([33,450,0.0003,0.08])\n",
    "plt.show()\n",
    "\n",
    "# Fit the degree distribution with a power law\n",
    "fit_function = pwl.Fit(list(AB_degree), xmin=11)\n",
    "\n",
    "# Output parameters\n",
    "print(\"alpha = \",fit_function.power_law.alpha)\n",
    "print(\"sigma = \",fit_function.power_law.sigma)\n",
    "print(\"x_min = \",fit_function.power_law.xmin)\n",
    "print(\"Kolmogorov-Smirnov distance = \",fit_function.power_law.D)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### G vs. WS\n",
    "\n",
    "A Watts-Strogatz graph combines small world (short average shortest path) with high clustering coefficient. This model starts from a lattice where each node is connected to its $d$ nearest neighbors,. and then with probability $r = 0.2$ each link is detached from one end and reformed with another random node.\n",
    "Let's compare G with random **Watts-Strogatz** graph with the same average connectivity and number of nodes.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's find the rewiring rate r that best approximates G, in terms of average clustering coefficient.\n",
    "n = U.number_of_nodes()                              # nodal cardinality\n",
    "d = 2*int(U.number_of_edges() / U.number_of_nodes())\n",
    "avg_clust_coeffs_ws = []\n",
    "r_log_list = numpy.logspace(-5, 0, num=20, endpoint=True, base=10.0, dtype=None, axis=0)\n",
    "#print(r_list)\n",
    "runs = 50\n",
    "for r in r_log_list:\n",
    "    WS = nx.connected_watts_strogatz_graph(n, d, r, runs)\n",
    "    #WS = nx.watts_strogatz_graph(n, d, r)\n",
    "    avg_clust_coeffs_ws.append(nx.average_clustering(WS))\n",
    "    #print(\"done 1\")\n",
    "   \n",
    "avg_clust_coeffs_ws_norm = [avg_clust_coeffs_ws[i]/avg_clust_coeffs_ws[0] for i in range(len(avg_clust_coeffs_ws))]\n",
    "\n",
    "plt.scatter(r_log_list,avg_clust_coeffs_ws_norm , marker = \"o\")\n",
    "plt.xscale(\"log\")\n",
    "plt.xlabel(\"$r$\")\n",
    "plt.axis([0.000005,1.32,0,1.1])\n",
    "plt.ylabel(\"$C(r)/C(0)$\")\n",
    "plt.title(\"\")\n",
    "plt.show()\n",
    "\n",
    "#find r value that best approximates G (in terms ofclustering coefficient; we couldn't choose the best compromise between average clustering coefficient and average shortest distance because the latter wouldhave taken too much time to evaluate for all r's)\n",
    "best_avg_cc = avg_clust_coeffs_ws[np.argmin([abs(avg_clust_coeffs_ws[i]-G_avg_cc) for i in range(len(avg_clust_coeffs_ws))])]\n",
    "best_r  = r_log_list[np.argmin([abs(avg_clust_coeffs_ws[i]-G_avg_cc) for i in range(len(avg_clust_coeffs_ws))])]\n",
    "\n",
    "print(\"best rewiring rate = \",best_r, \"\\nbest_avg_cc = \",best_avg_cc ,\"(\",abs(best_avg_cc-G_avg_cc),\"apart from G's one)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input parameters\n",
    "r = best_r\n",
    "\n",
    "WS = nx.connected_watts_strogatz_graph(n, d, r, runs)\n",
    "\n",
    "# Test connectedness\n",
    "print(\"Is the WS graph simply connected ?\", nx.is_connected(WS))\n",
    "\n",
    "# Connectivity\n",
    "print(\"The WS graph has\", WS.number_of_nodes(), \"nodes\", \"and\",WS.number_of_edges(),\"edges .\") \n",
    "\n",
    "# Average clustering coefficient\n",
    "print(\"The average clustering coefficient of WS is\", nx.average_clustering(WS))\n",
    "\n",
    "# Total number of triangles \n",
    "print(\"The global clustering coefficient is\", nx.transitivity(WS))\n",
    "\n",
    "# Average shortest path\n",
    "print(\"The WS graph has average shortest path  = \", nx.average_shortest_path_length(WS),\".\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the degree distribution \n",
    "ws_degrees = (dict(WS.degree()).values())\n",
    "\n",
    "# Plot the degree frequency distribution\n",
    "plt.figure(figsize=(7,4))\n",
    "plt.hist(ws_degrees, bins=12)\n",
    "plt.xlabel('$k$', fontsize=18)\n",
    "plt.ylabel('$P(k)$', fontsize=18)\n",
    "plt.xticks(fontsize=14)\n",
    "plt.yticks(fontsize=14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Degree Assortativity of a Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A network is assortative with respect to a feature/features if nodes with similar feature(s) values are more often connected between them rather then with nodes having different feature(s) values.<br>\n",
    "The degree assortativity is assortativity with respect to degree: are nodes with similar degree more connected between themselves than with nodes with different degree?<br>\n",
    "Degree assortativity can be measured in different ways. Using scalar assortativity theory we get the following quantities:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# degree assortativity can also be computed with nx's functions\n",
    "# Compute the degree assortativity coefficient of G and ER\n",
    "dac_G = nx.degree_assortativity_coefficient(G) # this is the pearson correlation coefficient of the red dots of the plot above. Infact, for the ER network it is close to zero, since in a ER network nodes are likely to connect regardless of their degree.\n",
    "dac_ER = nx.degree_assortativity_coefficient(ER)\n",
    "dac_AB = nx.degree_assortativity_coefficient(AB)\n",
    "dac_WS = nx.degree_assortativity_coefficient(WS)\n",
    "\n",
    "print(\"The degree assortativity coefficient of G is\", dac_G, \n",
    "      \"\\nwhile the degree assortativity coeffiecient of a ER graph is\", dac_ER,\n",
    "      \"\\nwhile the degree assortativity coeffiecient of a AB graph is\" ,dac_AB,\n",
    "      \"\\nwhile the degree assortativity coeffiecient of a WS graph is\" ,dac_WS,)\n",
    "\n",
    "# Compute the Pearson / linear correlation coefficient with nx function\n",
    "pcc_G = nx.degree_pearson_correlation_coefficient(G)\n",
    "pcc_ER = nx.degree_pearson_correlation_coefficient(ER)\n",
    "pcc_AB = nx.degree_pearson_correlation_coefficient(AB)\n",
    "pcc_WS = nx.degree_pearson_correlation_coefficient(WS)\n",
    "\n",
    "print(\"The Pearson correlation coefficient of G is\", pcc_G, \n",
    "      \"\\nwhile the Pearson correlation coeffiecient of a ER graph is\", pcc_ER,\n",
    "      \"\\nwhile the Pearson correlation coeffiecient of a AB graph is\" ,pcc_AB,\n",
    "      \"\\nwhile the Pearson correlation coeffiecient of a WS graph is\" ,pcc_WS,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Anyway, this approach does not take into consideration possible nonlinear degree correlations.\n",
    "A less powerful but more general approach would be to measure the average nearest neighbor degree per degree class, in order to determine a possible trend and to compare it with the expected average nearest neighbor degree per degree class if the network is uncorrelated, in which case:\n",
    "\n",
    "$$ k_{nn}^{unc}(k) = \\frac{<k^2>}{<k>}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the average nearest neighbour degree for all the nodes in G \n",
    "x=[]\n",
    "y=[]\n",
    "\n",
    "avg_knn = defaultdict(list)\n",
    "\n",
    "# so for every node n, extract its degree k and append to the value of the defaultdict avg_knn corresponding to the key k the average degree of the neighbours of that node. SO avg_knn becomes a dict of the type {k:[ak_1,ak_2,..]} where ak_i is the average degree of the neighbours of  the i-th node with degree k. Also save the k's in x and the average degrees of neighbours in y\n",
    "for n in G.nodes():\n",
    "    \n",
    "    #k=soc.omit_by(dct = dict(G.degree()))\n",
    "    k = G.degree(n)\n",
    "    #nn=len(G.neighbors(n))\n",
    "    total=0\n",
    "    if k != 0:\n",
    "        for j in G.neighbors(n):\n",
    "            total += G.degree(j)\n",
    "\n",
    "        avg_knn[k].append(float(total)/k)\n",
    "        x.append(k)\n",
    "        y.append(float(total)/k)\n",
    "    else:\n",
    "        avg_knn[k].append(0)\n",
    "        x.append(k)\n",
    "        y.append(0)\n",
    "    \n",
    "avg_knn_sort = {i:np.mean(avg_knn[i]) for i in sorted(avg_knn.keys())}\n",
    "degree_distrib = {k:sum([1 if k == undirected_degree_distribution[i] else 0 for i in undirected_degree_distribution.keys()])/len(G) for k in np.unique(list(undirected_degree_distribution.values()))}\n",
    "degrees = list(degree_distrib.keys())\n",
    "probs = list(degree_distrib.values())\n",
    "#k_unc = <k^2>/<k>\n",
    "k_unc = sum([(degrees[k]**2)*probs[k] for k in range(len(degrees))])/sum([(degrees[k])*probs[k] for k in range(len(degrees))])\n",
    "print(\"k_unc = \", k_unc)\n",
    "\n",
    "\n",
    "# Plot scatter average nearest neighbour degree per node and average degree connectivity and expected uncorrelated average neighbour degree per degree class  vs. individual degree\n",
    "knn_avg4_items = nx.average_degree_connectivity(G).items()\n",
    "knn_avg4 = sorted(knn_avg4_items) # same as avg_knn_sort\n",
    "z = [t[1] for t in knn_avg4]\n",
    "\n",
    "plt.figure(figsize=(10,7))\n",
    "plt.scatter(x,y, label='$k_{nn,i}$')\n",
    "plt.hlines(k_unc, 0, 500, colors='green', linestyles='solid', label='$k^{unc}_{nn}$', data=None)\n",
    "plt.plot(sorted(avg_knn.keys()), z,'r-', label='$k_{nn}(k)$')\n",
    "#plt.plot(sorted(avg_knn.keys()), z1,'g-')\n",
    "plt.legend(loc = 'center left', fontsize = 15)\n",
    "plt.xlabel('$k_i$', fontsize=18)\n",
    "plt.ylabel('$k_{nn}$', fontsize=18)\n",
    "plt.xticks(fontsize=14)\n",
    "plt.yticks(fontsize=14)\n",
    "plt.yscale('log')\n",
    "plt.xscale('log')\n",
    "plt.title('Degree Assortativity Analysis', fontsize = 17)\n",
    "# plt.axis([0.1,1000,1,1000])\n",
    "plt.axis([0.1,600,1,270])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rank_nodes = list of nodes to remove (one by one, progressively)\n",
    "# returns a list of tuples ( number of removed nodes, size of gcc) where gcc is the giant connected component at that moment. The decay of the size of gcc is a measure of the robustness of the network, and/or ogf the efficiency of the attack. If importnat nodes are removed, size of gcc willl dcrease rapidly, while if less important nodes are removed, gcc size will decrease with the number of (less important) nodes removed.\n",
    "def net_attack(graph, ranked_nodes):\n",
    "    \n",
    "    fraction_removed=[]#here we store the tuples: (%removed nodes, size of gcc)\n",
    "    \n",
    "    # make a copy of the graph to attack\n",
    "    graph1=graph.copy()\n",
    "    \n",
    "    nnodes=len(ranked_nodes)\n",
    "    n=0    \n",
    "    \n",
    "    gcc=list(nx.weakly_connected_components(graph1))[0] # we chose the weakly connected components beacuse the correspondig gcc has a relative size of 0.99, thus is representative of the network. Strong connection would give a relative size of 0.00..\n",
    "    \n",
    "    gcc_size=float(len(gcc))/nnodes\n",
    "    #print(\"gcc_size = \", gcc_size)\n",
    "    fraction_removed.append( (float(n)/nnodes, gcc_size) )\n",
    "    \n",
    "    while gcc_size>0.01:\n",
    "        \n",
    "        #we start from the end of the list!\n",
    "        graph1.remove_node(ranked_nodes.pop())\n",
    "\n",
    "        gcc=list(nx.weakly_connected_components(graph1))[0]\n",
    "        gcc_size=float(len(gcc))/nnodes\n",
    "        n+=1\n",
    "        fraction_removed.append( (float(n)/nnodes, gcc_size) )\n",
    "    \n",
    "    return fraction_removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the attck sequence is the list of the airports in no particular order\n",
    "nodes=list(G.nodes())\n",
    "resilience_random=net_attack(G, nodes)\n",
    "\n",
    "nodes_betw=[]\n",
    "\n",
    "betw=nx.betweenness_centrality(G)\n",
    "for i in sorted(betw.items(), key=itemgetter(1)):\n",
    "    nodes_betw.append(i[0])\n",
    "\n",
    "resilience_betw=net_attack(G, nodes_betw)\n",
    "\n",
    "nodes_degree=[]\n",
    "\n",
    "deg=dict(G.degree())\n",
    "for i in sorted(deg.items(), key=itemgetter(1)):\n",
    "    nodes_degree.append(i[0])\n",
    "\n",
    "resilience_deg=net_attack(G, list(nodes_degree))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=[k[0] for k in resilience_random]\n",
    "y=[k[1] for k in resilience_random]\n",
    "\n",
    "x1=[k[0] for k in resilience_deg]\n",
    "y1=[k[1] for k in resilience_deg]\n",
    "\n",
    "x2=[k[0] for k in resilience_betw]\n",
    "y2=[k[1] for k in resilience_betw]\n",
    "\n",
    "plt.figure(figsize=(10,7))\n",
    "\n",
    "plt.plot(x,y, label='Random Attack')\n",
    "plt.plot(x1,y1, label='Degree-based Attack')\n",
    "plt.plot(x2,y2, label='Betweenness-based Attack')\n",
    "\n",
    "plt.xlabel('$f_{c}$', fontsize=22)\n",
    "plt.ylabel('$LCC$', fontsize=22)\n",
    "plt.xticks(fontsize=20)\n",
    "plt.yticks(fontsize=22)\n",
    "\n",
    "plt.legend(loc='upper right', fontsize=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stochastic SIR Epidemic on Static Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Parameters\n",
    "mu = 0.2           # Recovery rate \n",
    "lambd = 0.01       # Transmission rate per contact\n",
    "\n",
    "# Simulation Parameters\n",
    "nrun = 1000        # Number of runs\n",
    "\n",
    "# Multi-Run Simulation\n",
    "runs = soc.network_SIR_multirun_simulation(G, nrun = nrun, lambd = lambd, mu = mu)\n",
    "\n",
    "# Set figure size \n",
    "plt.figure(figsize=(10,7))\n",
    "\n",
    "# Plot the ensemble of trajectories\n",
    "soc.plot_ensemble(runs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $\\lambda$-Sensitivity of Final Epidemic Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform lambda-sensitivity analysis of final epidemic size (normalized attack rate) \n",
    "data = soc.network_SIR_finalsize_lambda_sensitivity(G, mu = mu, rho = 0.05, # rho = initial fraction infected\n",
    "                                                    lambda_min = 0.0001, lambda_max = 1.0, \n",
    "                                                    nruns = 20)\n",
    "# Show sensitivity dataset\n",
    "data \n",
    "\n",
    "# Set figure size \n",
    "plt.figure(figsize=(10,7))\n",
    "\n",
    "# Display a boxplot with final epidemic size vs. transmission rate per edge/contact\n",
    "soc.boxplot_finalsize_lambda_sensitivity(G, mu = mu, data = data, \n",
    "                                         ymin = 0.045, ymax= 1.1,\n",
    "                                         xlim = (0.00007, 1.5) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Community Detection "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modularity | `Gephi`\n",
    "\n",
    "* **Algorithm**: Vincent D Blondel et al. [Fast unfolding of communities in large networks](https://iopscience.iop.org/article/10.1088/1742-5468/2008/10/P10008/meta), *Journal of Statistical Mechanics: Theory and Experiment* (2008).\n",
    "\n",
    "* **Resolution**: R. Lambiotte et al.[Barahona Laplacian Dynamics and Multiscale Modular Structure in Networks](https://arxiv.org/abs/0812.1770), *arXiv pre-print* (2009).\n",
    "\n",
    "#### Inputs \n",
    "\n",
    "* Randomize: On\n",
    "* Use edge weights: On\n",
    "\n",
    "#### Low Resolution (0.5)\n",
    "\n",
    "* Modularity: 0.390\n",
    "* Modularity with resolution: 0.131\n",
    "* Number of Communities: 13\n",
    "\n",
    "![Modularity Report](./Gephi/Modularity_0.5/communities-size-distribution.png) <img src=\"./Images/Twitter/@PietroMonticone0.5.png\" alt=\"Visualization with color coded communities\" width=\"550\"/>\n",
    "\n",
    "#### Medium Resolution (1.2)\n",
    "\n",
    "* Modularity: 0.442\n",
    "* Modularity with resolution: 0.582\n",
    "* Number of Communities: 7\n",
    "\n",
    "![Modularity Report](./Gephi/Modularity_1.2/communities-size-distribution.png) <img src=\"./Images/Twitter/@PietroMonticone1.2.png\" alt=\"Visualization with color coded communities\" width=\"450\"/>\n",
    "\n",
    "#### High Resolution (3.9)\n",
    "\n",
    "* Modularity: 0.000\n",
    "* Modularity with resolution: 2.900\n",
    "* Number of Communities: 3\n",
    "\n",
    "![Modularity Report](./Gephi/Modularity_3.9/communities-size-distribution.png) <img src=\"./Images/Twitter/@PietroMonticone3.9.png\" alt=\"Visualization with color coded communities\" width=\"450\"/>\n",
    "\n",
    "### Girvan-Newman Clustering \n",
    "\n",
    "1. Create a partition sequence\n",
    "  1. Calculate the betweenness centrality for all links.\n",
    "  2. Remove the link with largest betweenness and create a partition using connected components.\n",
    "  3. Recalculate the betweenness centrality of the links of the resulting graph.\n",
    "  4. Repeat from step B until no links remain.\n",
    "2. Evaluate each partition in the sequence and choose the one with the highest modularity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def updateGraph1(G):\n",
    "    ebw = nc.edge_betweenness(G)\n",
    "    maxs = 0\n",
    "    for k, v in ebw.items():\n",
    "        if maxs < v:\n",
    "            medge, maxs = k, v\n",
    "    G.remove_edge(medge[0],medge[1])\n",
    "\n",
    "def updateGraph2(G):\n",
    "    ebw = nc.edge_betweenness(G)\n",
    "    edge_list=sorted(ebw.items(), key=itemgetter(1))\n",
    "    medge=edge_list[-1][0]\n",
    "    G.remove_edge(medge[0],medge[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected the Girvan-Nirvan algorithm doesn't scale well with edge cardinality of the graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G_weakly_cc = list(nx.weakly_connected_components(G))\n",
    "GWCC = list(G_weakly_cc[0])\n",
    "WCC = G.subgraph(GWCC)\n",
    "\n",
    "# Resolution: 1.2\n",
    "# Modularity: 0.442\n",
    "# Number of Communities: 7\n",
    "    \n",
    "n_wcc = nx.number_weakly_connected_components(WCC)\n",
    "\n",
    "while nx.number_weakly_connected_components(WCC) < 8:\n",
    "    updateGraph2(WCC)\n",
    "    if nx.number_weakly_connected_components(WCC) !=  n_wcc:\n",
    "        print(nx.number_weakly_connected_components(WCC))\n",
    "        n_wcc = nx.number_weakly_connected_components(WCC)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
