{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First Order Ego Graph Analysis: Facebook User 2\n",
    "\n",
    "In this notebook we analyze the first order egograph of a Facebook account with $\\sim10^3$ friends."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable interactive numpy and matplotlib\n",
    "%pylab inline\n",
    "\n",
    "# Data Wrangling \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Data Analysis\n",
    "import powerlaw as pwl\n",
    "\n",
    "# Data Visualization\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt \n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "# Network Analysis \n",
    "import networkx as nx\n",
    "from networkx.algorithms import community\n",
    "import networkx.algorithms.centrality as nc\n",
    "import social_physics as soc\n",
    "\n",
    "# Network Epidemiology \n",
    "import EoN\n",
    "\n",
    "# Data Visualization\n",
    "import seaborn as sns\n",
    "from netwulf import visualize\n",
    "\n",
    "# Other Utilities \n",
    "import sys, os, os.path\n",
    "import itertools\n",
    "from progressbar import ProgressBar, Bar, Percentage\n",
    "from operator import itemgetter\n",
    "from collections import Counter\n",
    "from collections import defaultdict\n",
    "import random\n",
    "\n",
    "# Reload Custom Modules\n",
    "from importlib import reload\n",
    "soc = reload(soc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graph Data Collection\n",
    "\n",
    "Import the (undirected) graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import graphml file \n",
    "G = nx.Graph(nx.read_graphml(\"/Users/pietromonticone/github/SocialPhysicsProject/Data/GraphML/Facebook2.graphml\"))\n",
    "\n",
    "# Rename the graph \n",
    "G.name = \"Facebook Friend Ego Graph\"\n",
    "\n",
    "# Show the basic attributes of the graph\n",
    "print(nx.info(G))\n",
    "\n",
    "# Relable the nodes (from strings of Twitter IDs to integers)\n",
    "G = nx.convert_node_labels_to_integers(G, first_label=0, ordering='default', label_attribute=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graph Visualization "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python \n",
    "visualize(G)\n",
    "```\n",
    "\n",
    "![](./Images/Facebook/Facebook2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graph Data Analysis \n",
    "\n",
    "Let's visulize normalized degree distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Degree Distribution "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get degree distribution \n",
    "undirected_degree_distribution, degree_mean, degree_variance = soc.get_degree_distribution(G, \"degree\")\n",
    "\n",
    "# Set figure size\n",
    "plt.figure(figsize=(9,6))\n",
    "\n",
    "# Plot undirected degree distribution \n",
    "soc.plot_degree_distribution(undirected_degree_distribution, \n",
    "                             title = \"Degree Distribution\", \n",
    "                             log = False, \n",
    "                             display_stats = True)\n",
    "\n",
    "# Show mean and variance of the undirected degree distribution \n",
    "print(\"Mean = \", degree_mean,\"\\nVar = \", degree_variance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logarithmic Binning\n",
    "The black line is the empirical linearly binned pdf. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set figure size\n",
    "plt.figure(figsize=(7,4))\n",
    "\n",
    "# Plot\n",
    "pwl_distribution = soc.power_law_plot(graph = G, log = True,linear_binning = False, bins = 1000, draw = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The empirical PDF doesn't interpolate because it is obtained via linear binning while the red data points represent the logarithmic binning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Binning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set figure size\n",
    "plt.figure(figsize=(7,4))\n",
    "\n",
    "# Plot \n",
    "pwl_distribution = soc.power_law_plot(graph = G, log = True,linear_binning = True, bins = 90, draw = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The figure above interpolates because it uses linear binnig both for scatter plot and pdf binning. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Power Law Fitting \n",
    "\n",
    "Here we estimate the measure to which the network follows a power law, and compare it with other common distributions.\n",
    "\n",
    "#### Parameters Estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_function = pwl.Fit(list(undirected_degree_distribution.values())) \n",
    "\n",
    "\n",
    "print(\"Exponent = \", fit_function.power_law.alpha)\n",
    "print(\"Sigma (error associated to exponent) = \",fit_function.power_law.sigma)\n",
    "xmin = fit_function.power_law.xmin\n",
    "print(\"x_min = \",xmin)\n",
    "print(\"Kolmogorov-Smirnov distance = \",fit_function.power_law.D)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because the fitted $x_{min} = 30$, let's require it to be a little higher prior to fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_function_fix_xmin = pwl.Fit(list(undirected_degree_distribution.values()),xmin= 5) \n",
    "print(\"Exponent = \", fit_function_fix_xmin.power_law.alpha)\n",
    "print(\"Sigma (error associated to exponent) = \",fit_function_fix_xmin.power_law.sigma)\n",
    "print(\"x_min = \",fit_function_fix_xmin.power_law.xmin)\n",
    "print(\"Kolmogorov-Smirnov distance = \",fit_function_fix_xmin.power_law.D)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the error (sigma) is way lower than before, but Kolmogorov-Smironv is higher as expected (because we fixed $x_{min}$ prior to fitting). Thus we confirmed that a power law fitting is rather good for this network. It is to be recalled that power laws are usually able to explain most of the variance though.<br>\n",
    "Let us now compare the actual pdf with the fitted power law near the tail.<br>\n",
    "<span style=\"color:blue\">BLUE</span> : Fitted power law <br>\n",
    "<span style=\"color:black\">BLACK</span> : plotted pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set figure size\n",
    "plt.figure(figsize=(7,4))\n",
    "\n",
    "pwl_distribution = soc.power_law_plot(graph = G, log = True, linear_binning = False, bins = 90, draw = True, x_min = xmin)\n",
    "\n",
    "fit_function.power_law.plot_pdf(color='b', linestyle='-', linewidth=1)\n",
    "\n",
    "plt.xticks(fontsize=14)\n",
    "plt.yticks(fontsize=14)\n",
    "plt.xlabel('$k$', fontsize=16)\n",
    "plt.ylabel('$P(k)$', fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, let's plot the power law fitted with the $x_{min}$ fixed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set figure size\n",
    "plt.figure(figsize=(7,4))\n",
    "\n",
    "#plt.plot(x,y,'ro')\n",
    "\n",
    "pwl_distribution = soc.power_law_plot(graph = G, log = True,linear_binning = False, bins = 90, draw = True, x_min = xmin)\n",
    "\n",
    "fit_function_fix_xmin.power_law.plot_pdf(color='b', linestyle='-', linewidth=1)\n",
    "\n",
    "#fig.legend(fontsize=22)\n",
    "plt.xticks(fontsize=14)\n",
    "plt.yticks(fontsize=14)\n",
    "plt.xlabel('$k$', fontsize=16)\n",
    "plt.ylabel('$P(k)$', fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now compare power law against other probability distributions. Remember that $R$ is the log-likelihood ratio between the two candidate distributions which will be positive if the data is more likely in the first distribution, and negative if the data is more likely in the second distribution. The significance value for that direction is $p$ (the smaller the better). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "R,p = fit_function.distribution_compare('power_law', 'exponential', normalized_ratio=True) \n",
    "R,p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "R,p = fit_function.distribution_compare('power_law', 'lognormal_positive', normalized_ratio=True)\n",
    "R,p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "R,p = fit_function.distribution_compare('power_law', 'truncated_power_law', normalized_ratio=True)\n",
    "R,p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "R,p = fit_function.distribution_compare('power_law', 'stretched_exponential', normalized_ratio=True)\n",
    "R,p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us also compare with the truncated power law:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "R,p = fit_function_fix_xmin.distribution_compare('power_law', 'exponential', normalized_ratio=True) \n",
    "R,p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "R,p = fit_function_fix_xmin.distribution_compare('power_law', 'lognormal_positive', normalized_ratio=True) \n",
    "R,p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "R,p = fit_function_fix_xmin.distribution_compare('power_law', 'stretched_exponential', normalized_ratio=True) \n",
    "R,p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Centrality Metrics \n",
    "\n",
    "Now we turn to plot centralities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Degree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get centrality \n",
    "degree_centrality = soc.get_centrality(G, \"degree\")\n",
    "\n",
    "# Set figure size\n",
    "plt.figure(figsize=(7,4))\n",
    "\n",
    "# Plot centrality distribution\n",
    "soc.plot_centrality_distribution(G, degree_centrality, \"Blue\", 15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Closeness\n",
    "\n",
    "In connected graphs there is a natural distance metric between all pairs of nodes, defined by the length of their shortest paths. \n",
    "The '''farness''' of a node ''x'' is defined as the sum of its distances from all other nodes, and its closeness was defined by Bavelas as the reciprocal of the farness that is:\n",
    "\n",
    "\n",
    "<center>\n",
    "$C(x)= \\frac{1}{\\sum_y d(y,x)}.$\n",
    "</center>\n",
    "\n",
    "\n",
    "Thus, the more central a node is the lower its total distance from all other nodes. Note that taking distances ''from'' or ''to'' all other nodes is irrelevant in undirected graphs, whereas in directed graphs distances ''to'' a node are considered a more meaningful measure of centrality, as in general (e.g., in, the web) a node has little control over its incoming links."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get centrality (computationally intensive!)\n",
    "closeness_centrality = soc.get_centrality(G, \"closeness\")\n",
    "\n",
    "# Set figure size\n",
    "plt.figure(figsize=(7,4))\n",
    "\n",
    "# Plot centrality distribution\n",
    "soc.plot_centrality_distribution(G, closeness_centrality, \"Blue\", 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bewteenness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get centrality \n",
    "betweenness_centrality = soc.get_centrality(G, \"betweenness\")\n",
    "\n",
    "# Set figure size\n",
    "plt.figure(figsize=(7,4))\n",
    "\n",
    "# Plot centrality distribution\n",
    "soc.plot_centrality_distribution(G, betweenness_centrality, \"Blue\", 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Katz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get centrality \n",
    "katz_centrality = soc.get_centrality(G, \"katz\")\n",
    "\n",
    "# Set figure size\n",
    "plt.figure(figsize=(7,4))\n",
    "\n",
    "# Plot centrality distribution\n",
    "soc.plot_centrality_distribution(G, katz_centrality, \"Blue\", 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Eigenvector "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get centrality \n",
    "eigenvector_centrality = soc.get_centrality(G, \"eigenvector\")\n",
    "\n",
    "# Set figure size\n",
    "plt.figure(figsize=(7,4))\n",
    "\n",
    "x_centrality=[]\n",
    "y_centrality=[]\n",
    "     \n",
    "for i in eigenvector_centrality:\n",
    "    x_centrality.append(i[0])\n",
    "    y_centrality.append(i[1])\n",
    "\n",
    "plt.scatter(x_centrality,y_centrality, color=\"Blue\", marker=\"o\",alpha=0.50) \n",
    "#plt.yscale('log')\n",
    "#plt.xscale('log')\n",
    "plt.xlabel('$x$', fontsize = 15)\n",
    "plt.ylabel('$P(x)$', fontsize = 15)\n",
    "plt.xticks(fontsize=12)\n",
    "plt.yticks(fontsize=12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PageRank\n",
    "$$x_i=(1-\\alpha) \\sum_{j}A^{T}_{ij}\\frac{x_j}{k^{out}_j}+\\frac{\\alpha}{N}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get centrality \n",
    "pagerank_centrality = soc.get_centrality(G, \"pagerank\")\n",
    "\n",
    "# Set figure size\n",
    "plt.figure(figsize=(7,4))\n",
    "\n",
    "# Plot centrality distribution\n",
    "soc.plot_centrality_distribution(G, pagerank_centrality, \"Blue\", 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connectivity "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the connectivity of the analyzed graph\n",
    "print(\"The graph has\", G.number_of_nodes(), \"nodes and\", G.number_of_edges(),\"edges.\")\n",
    "\n",
    "print(\"Is the graph connected?\", nx.is_connected(G),\".\")\n",
    "G_cc = sorted(list(nx.connected_components(G)),key=len, reverse=True)\n",
    "print(\"The graph has\", len(G_cc),\"connected components.\")\n",
    "print(\"The sizes of the connected components are\", [len(c) for c in sorted(G_cc, key=len, reverse=True)],\". \\nThus the GCC represents \", len(G_cc[0])/len(G), \" of the nodal cardinality.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering \n",
    "\n",
    "Below the evaluation of the *average clustering coefficient* and the *global clustering coefficient* may be found."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Global clustering coefficient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The global clustering coefficient measures the number of triangles in the network and it's defined as\n",
    "\n",
    "$$ C_\\Delta = \\frac{3 \\times \\text{triangles}}{\\text{triplets}} $$\n",
    "\n",
    "In order to compare our graph with theorical models (of the same size), it is thus sufficient to evaluate the number of triangles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the global clustering coefficient of U (the fraction of all possible triangles in the network)\n",
    "print(\"Global clustering coefficient = \", nx.transitivity(G))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Average clustering coefficient\n",
    "\n",
    "The overall level of clustering in a network is measured by Watts and Strogatz as the average of the local clustering coefficients of all the vertices $n$:\n",
    "\n",
    "$$\\bar{C} = \\frac{1}{n}\\sum_{i=1}^{n} C_i.$$\n",
    "\n",
    "\n",
    "It is worth noting that this metric places more weight on the low degree nodes, while the transitivity ratio places more weight on the high degree nodes. In fact, a weighted average where each local clustering score is weighted by $k_i(k_i-1)$ is identical to the global clustering coefficient.\n",
    "<br> As per [this](https://networkx.github.io/documentation/stable/reference/algorithms/generated/networkx.algorithms.cluster.clustering.html) and [this](https://networkx.github.io/documentation/stable/_modules/networkx/algorithms/cluster.html ) resources we notice that Networkx's `average_clustering` function automatically takes care of the network being directed or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G_avg_cc =  nx.average_clustering(G)\n",
    "print(\"The average clustering coefficient is \", G_avg_cc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Path-ology  \n",
    "\n",
    "### Average shortest path length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GWCC = list(G_cc[0])\n",
    "print(\"Since the graph is not connected, but one of its 3 weakly connected compoments amounts for \",len(GWCC)/len(G), \"of the nodes count, we approximate its averaege shortest path length with that of its bigger connected component, which is:\", nx.average_shortest_path_length(G.subgraph(GWCC)), \"\\nLet's compare it with lnlnN = \", math.log(math.log(len(GWCC))), \"(ultra small world)\\nand with lnN/lnlnN = \", math.log(len(GWCC))/math.log(math.log(len(GWCC))), \"(equivalent to a power law with exponent 3)\\nand with lnN/ln(<k>) = \", math.log((len(GWCC)))/math.log(27.9394), \"equivalent to a random network.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparisons\n",
    "\n",
    "#### G vs. ER\n",
    "\n",
    "The most natural benchmark is a ER (random) network with the same number of nodes and links. In a ER netork, the p_k is poissonian ( an exponential decay) , so let's compare G with random **Erdos-Renyi** graph with the same average connectivity and number of nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nnodes = G.number_of_nodes()\n",
    "nedges = G.number_of_edges()\n",
    "#plink = 0.07803\n",
    "plink = 2*nedges/(nnodes*(nnodes-1)) #2* because it is undirected\n",
    "\n",
    "ER = nx.fast_gnp_random_graph(nnodes, plink)\n",
    "\n",
    "average_degree = sum(list(dict(ER.degree()).values()))/len(ER.degree())\n",
    "\n",
    "# Connectivity\n",
    "print(\"The ER graph has\", len(ER), \"nodes\", \"and\",len(ER.edges()),\"edges.\\n The difference between its maximum and minimun degree is:\",max(list(dict(ER.degree).values()))-min(list(dict(ER.degree).values())),\", while the sane difference in our network is:\", max(list(dict(G.degree).values()))-min(list(dict(G.degree).values())),\"which is higher, confirming that real nertworks are not random.\")\n",
    "\n",
    "# Test connectedness\n",
    "print(\"Is the ER graph simply connected ?\", nx.is_connected(ER), \". Infact the average degree is:\",\n",
    "      average_degree,\"and the natural log of the number of nodes is\", \n",
    "      math.log(nnodes),\"which is smaller, then we are in the connected regime.\")\n",
    "\n",
    "# Average clustering coefficient\n",
    "print(\"The average clustering coefficient of ER is\", nx.average_clustering(ER),\"which, if compared with <k>/N\", average_degree/(nnodes), \"we can observe they are similar as expected. But it is approximately one order of magnitude less than the egonetwork's one. \")\n",
    "\n",
    "# Total number of triangles\n",
    "print(\"The transitivity of the network is\", nx.transitivity(ER))\n",
    "\n",
    "# Average shortest path\n",
    "print(\"The ER graph is small world since the average shortest path is\", nx.average_shortest_path_length(ER))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### G vs. AB\n",
    "\n",
    "Thinking about  a broad (not exponential decaying) distribution, more like a power law, we may think about a AB network (albert-barabasi), so let's compare G with random **Albert-Barabasi** graph with the same average connectivity and number of nodes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = G.number_of_nodes()\n",
    "m = int(G.number_of_edges() / G.number_of_nodes()) \n",
    "\n",
    "AB = nx.barabasi_albert_graph(n,m)\n",
    "\n",
    "# Test connectedness\n",
    "print(\"Is the AB graph simply connected ?\", nx.is_connected(AB))\n",
    "# Connectivity\n",
    "print(\"The AB graph has\", len(AB), \"nodes\", \"and\",len(AB.edges()),\"edges ..\\n The difference between its maximum and minimun degree is:\",max(list(dict(AB.degree).values()))-min(list(dict(AB.degree).values())), \", while the sane difference in our network is:\", max(list(dict(G.degree).values()))-min(list(dict(G.degree).values())), \"which is of similar order of magnitude, confirming that albert barabasi captures the fundamental mechanisms that underly real networj formation better than a random network would.\")\n",
    "\n",
    "# Average clustering coefficient\n",
    "print(\"The average clustering coefficient of AB is\", nx.average_clustering(AB),\". We may compare it with the predicted C_l = (m*ln(N)^2)/(4*N) = \",(m*(math.log(n))**2)/(4*n),\"while the global clustering coefficient is: \",nx.transitivity(AB),\".\")\n",
    "\n",
    "\n",
    "# Average shortest path\n",
    "print(\"The AB graph is small world since the average shortest path is\", nx.average_shortest_path_length(AB), \n",
    "     \"and the expeted result is lnN/lnlnN\", math.log(len(AB))/math.log(math.log(len(AB))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's verify that an AB network follows a power law distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the degree distribution\n",
    "AB_degree = dict(AB.degree()).values()\n",
    "AB_degree_distribution = Counter(AB_degree)\n",
    "\n",
    "# Plot the degree frequency distribution & \n",
    "# the probability density function \n",
    "plt.figure(figsize=(10,7))\n",
    "x=[]\n",
    "y=[]\n",
    "for i in sorted(AB_degree_distribution):   \n",
    "    x.append(i)\n",
    "    y.append(float(AB_degree_distribution[i])/len(AB))\n",
    "\n",
    "plt.plot(np.array(x),np.array(y))\n",
    "pwl.plot_pdf(list(AB_degree))\n",
    "\n",
    "plt.xlabel('$k$', fontsize=18)\n",
    "plt.ylabel('$P(k)$', fontsize=18)\n",
    "plt.xticks(fontsize=14)\n",
    "plt.yticks(fontsize=14)\n",
    "plt.yscale('log')\n",
    "plt.xscale('log')\n",
    "plt.axis([10,400,0.000,1])\n",
    "plt.show()\n",
    "\n",
    "# Fit the degree distribution with a power law\n",
    "fit_function = pwl.Fit(list(AB_degree), xmin=11)\n",
    "\n",
    "# Output parameters\n",
    "print(\"alpha = \",fit_function.power_law.alpha)\n",
    "print(\"sigma = \",fit_function.power_law.sigma)\n",
    "print(\"x_min = \", fit_function.power_law.xmin)\n",
    "print(\"Kolmogorov-Smirnov distance = \",fit_function.power_law.D)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### G vs. WS\n",
    "Watts stogatz netowrk combines small world (short average shortest path) with high clustering coefficient. This model starts from a reticule where each node is connected to its $d$ nearest neighbors,. and then with probability $r = 0.2$ each link is detached from one end and reformed with another random node.\n",
    "Let's compare G with random **Watts-Strogatz** graph with the same average connectivity and number of nodes.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's find the rewiring rate r that best approximates G, in terms of average clustering coefficient.\n",
    "n = G.number_of_nodes()                              # nodal cardinality\n",
    "d = 2*int(G.number_of_edges() / G.number_of_nodes())\n",
    "avg_clust_coeffs_ws = []\n",
    "r_log_list = numpy.logspace(-5, 0, num=20, endpoint=True, base=10.0, dtype=None, axis=0)\n",
    "#print(r_list)\n",
    "runs = 50\n",
    "for r in r_log_list:\n",
    "    WS = nx.connected_watts_strogatz_graph(n, d, r, runs)\n",
    "    #WS = nx.watts_strogatz_graph(n, d, r)\n",
    "    avg_clust_coeffs_ws.append(nx.average_clustering(WS))\n",
    "   \n",
    "avg_clust_coeffs_ws_norm = [avg_clust_coeffs_ws[i]/avg_clust_coeffs_ws[0] for i in range(len(avg_clust_coeffs_ws))]\n",
    "\n",
    "plt.scatter(r_log_list,avg_clust_coeffs_ws_norm , marker = \"o\")\n",
    "plt.xscale(\"log\")\n",
    "plt.xlabel(\"$r$\")\n",
    "plt.axis([0.000005,1.32,-0.1,1.1])\n",
    "plt.ylabel(\"$C(r)/C(0)$\")\n",
    "plt.title(\"\")\n",
    "plt.show()\n",
    "\n",
    "#find r value that best approximates G (in terms ofclustering coefficient; we couldn't choose the best compromise between average clustering coefficient and average shortest distance because the latter wouldhave taken too much time to evaluate for all r's)\n",
    "best_avg_cc = avg_clust_coeffs_ws[np.argmin([abs(avg_clust_coeffs_ws[i]-G_avg_cc) for i in range(len(avg_clust_coeffs_ws))])]\n",
    "best_r  = r_log_list[np.argmin([abs(avg_clust_coeffs_ws[i]-G_avg_cc) for i in range(len(avg_clust_coeffs_ws))])]\n",
    "\n",
    "print(\"best rewiring rate = \",best_r, \"\\nbest_avg_cc = \",best_avg_cc ,\"(\",abs(best_avg_cc-G_avg_cc),\"apart from G's one)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input parameters\n",
    "r = best_r\n",
    "\n",
    "WS = nx.connected_watts_strogatz_graph(n, d, r, runs)\n",
    "\n",
    "# Test connectedness\n",
    "print(\"Is the WS graph simply connected ?\", nx.is_connected(WS))\n",
    "\n",
    "# Connectivity\n",
    "print(\"The WS graph has\", len(WS), \"nodes\", \"and\",len(WS.edges()),\"edges .\")\n",
    "\n",
    "# Average clustering coefficient\n",
    "print(\"The average clustering coefficient of WS is\", nx.average_clustering(WS))\n",
    "\n",
    "# Total number of triangles \n",
    "print(\"The total number of triangles in the network is\", sum(list(nx.triangles(WS).values()))/3)\n",
    "\n",
    "# Average shortest path\n",
    "print(\"The WS graph has average shortest path  = \", nx.average_shortest_path_length(WS), \n",
    "     \"\\n.And we compare it with lnN/ln(<k>) = \", math.log(len(WS))/math.log(n*d/(n)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the degree distribution \n",
    "ws_degrees = (dict(WS.degree()).values())\n",
    "\n",
    "# Plot the degree frequency distribution\n",
    "plt.figure(figsize=(10,7))\n",
    "plt.hist(ws_degrees, bins=10)\n",
    "plt.xlabel('$k$', fontsize=18)\n",
    "plt.ylabel('$P(k)$', fontsize=18)\n",
    "plt.xticks(fontsize=14)\n",
    "plt.yticks(fontsize=14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Degree assortativity of a network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A network is assortative with respect to a feature/features if nodes with similar feature(s) values are more often connected between them rather then with nodes having different feature(s) values.<br>\n",
    "The degree assortativity is assortativity with respect to degree: are nodes with similar degree more connected between themselves than with nodes with different degree?<br>\n",
    "Degree assortativity can be measured in different ways. A simple approach is measuring the average nearest neighbor degree to assess the level of degree-assortativity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# degree assortativity can also be computed with nx's functions\n",
    "# Compute the degree assortativity coefficient of G and ER\n",
    "dac_G = nx.degree_assortativity_coefficient(G) # this is the pearson correlation coefficient of the red dots of the plot above. Infact, for the ER network it is close to zero, since in a ER network nodes are likely to connect regardless of their degree.\n",
    "dac_ER = nx.degree_assortativity_coefficient(ER)\n",
    "dac_AB = nx.degree_assortativity_coefficient(AB)\n",
    "dac_WS = nx.degree_assortativity_coefficient(WS)\n",
    "\n",
    "print(\"The degree assortativity coefficient of G is\", dac_G, \n",
    "      \"\\nwhile the degree assortativity coeffiecient of a ER graph is\", dac_ER,\n",
    "      \"\\nwhile the degree assortativity coeffiecient of a AB graph is\" ,dac_AB,\n",
    "      \"\\nwhile the degree assortativity coeffiecient of a WS graph is\" ,dac_WS,)\n",
    "\n",
    "# Compute the Pearson / linear correlation coefficient with nx function\n",
    "pcc_G = nx.degree_pearson_correlation_coefficient(G)\n",
    "pcc_ER = nx.degree_pearson_correlation_coefficient(ER)\n",
    "pcc_AB = nx.degree_pearson_correlation_coefficient(AB)\n",
    "pcc_WS = nx.degree_pearson_correlation_coefficient(WS)\n",
    "\n",
    "print(\"The Pearson correlation coefficient of G is\", pcc_G, \n",
    "      \"\\nwhile the Pearson correlation coeffiecient of a ER graph is\", pcc_ER,\n",
    "      \"\\nwhile the Pearson correlation coeffiecient of a AB graph is\" ,pcc_AB,\n",
    "      \"\\nwhile the Pearson correlation coeffiecient of a WS graph is\" ,pcc_WS,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Anyway, thsi approachd oes not take into consideration possible nonlinear degree correlations.\n",
    "A less powerful but more general approach would be to measure the average nearest neighbor degree per degree class, in order to determine a possible trend and to compar eit with the expected average nearest neighbor degree per degree class if the network is uncorrelated, in which case:\n",
    "\n",
    "$$ k_{nn}^{unc}(k) = \\frac{<k^2>}{<k>}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the average nearest neighbour degree for all the nodes in G \n",
    "x=[]\n",
    "y=[]\n",
    "\n",
    "avg_knn = defaultdict(list)\n",
    "\n",
    "# so for every node n, extract its degree k and append to the value of the defaultdict avg_knn corresponding to the key k the average degree of the neighbours of that node. SO avg_knn becomes a dict of the type {k:[ak_1,ak_2,..]} where ak_i is the average degree of the neighbours of  the i-th node with degree k. Also save the k's in x and the average degrees of neighbours in y\n",
    "for n in G.nodes():\n",
    "    \n",
    "    #k=soc.omit_by(dct = dict(G.degree()))\n",
    "    k = G.degree(n)\n",
    "    #nn=len(G.neighbors(n))\n",
    "    total=0\n",
    "    if k != 0:\n",
    "        for j in G.neighbors(n):\n",
    "            total += G.degree(j)\n",
    "\n",
    "        avg_knn[k].append(float(total)/k)\n",
    "        x.append(k)\n",
    "        y.append(float(total)/k)\n",
    "    else:\n",
    "        avg_knn[k].append(0)\n",
    "        x.append(k)\n",
    "        y.append(0)\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "avg_knn_sort = {i:np.mean(avg_knn[i]) for i in sorted(avg_knn.keys())}\n",
    "degree_distrib = {k:sum([1 if k == undirected_degree_distribution[i] else 0 for i in undirected_degree_distribution.keys()])/len(G) for k in np.unique(list(undirected_degree_distribution.values()))}\n",
    "degrees = list(degree_distrib.keys())\n",
    "probs = list(degree_distrib.values())\n",
    "#k_unc = <k^2>/<k>\n",
    "k_unc = sum([(degrees[k]**2)*probs[k] for k in range(len(degrees))])/sum([(degrees[k])*probs[k] for k in range(len(degrees))])\n",
    "print(\"k_unc = \", k_unc)\n",
    "\n",
    "\n",
    "# Plot scatter average nearest neighbour degree per node and average degree connectivity and expected uncorrelated average neighbour degree per degree class  vs. individual degree\n",
    "knn_avg4_items = nx.average_degree_connectivity(G).items()\n",
    "knn_avg4 = sorted(knn_avg4_items) # same as avg_knn_sort\n",
    "#print(type(knn_avg4),knn_avg4[0])\n",
    "z = [t[1] for t in knn_avg4]\n",
    "\n",
    "plt.figure(figsize=(10,7))\n",
    "plt.scatter(x,y, label='$k_{nn,i}$')\n",
    "plt.hlines(k_unc, 0, 500, colors='green', linestyles='solid', label='$k^{unc}_{nn}$', data=None)\n",
    "plt.plot(sorted(avg_knn.keys()), z,'r-', label='$k_{nn}(k)$')\n",
    "#plt.plot(sorted(avg_knn.keys()), z1,'g-')\n",
    "plt.legend(loc = 'lower left', fontsize = 15)\n",
    "plt.xlabel('$k_i$', fontsize=18)\n",
    "plt.ylabel('$k_{nn}$', fontsize=18)\n",
    "plt.xticks(fontsize=14)\n",
    "plt.yticks(fontsize=14)\n",
    "plt.yscale('log')\n",
    "plt.xscale('log')\n",
    "plt.title('Degree Assortativity Analysis', fontsize = 17)\n",
    "# plt.axis([0.1,1000,1,1000])\n",
    "plt.axis([0.1,800,1,500])\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rank_nodes = list of nodes to remove (one by one, progressively)\n",
    "# returns a list of tuples ( number of removed nodes, size of gcc) where gcc is the giant connected component at that moment. The decay of the size of gcc is a measure of the robustness of the network, and/or ogf the efficiency of the attack. If importnat nodes are removed, size of gcc will dcrease rapidly, while if less important nodes aare removed, gcc size will decrease with the number of (less important) nodes removed.\n",
    "def net_attack(graph, ranked_nodes):\n",
    "    \n",
    "    fraction_removed=[]#here we store the tuples: (%removed nodes, size of gcc)\n",
    "    \n",
    "    # make a copy of the graph to attack\n",
    "    graph1=graph.copy()\n",
    "    \n",
    "    nnodes=len(ranked_nodes)\n",
    "    n=0    \n",
    "    \n",
    "    gcc=sorted(list(nx.connected_components(graph1)),key=len, reverse=True)[0] \n",
    "    \n",
    "    gcc_size=float(len(gcc))/nnodes\n",
    "    print(\"gcc_size = \", gcc_size)\n",
    "    fraction_removed.append( (float(n)/nnodes, gcc_size) )\n",
    "    \n",
    "    while gcc_size>0.01:\n",
    "        \n",
    "        #we start from the end of the list!\n",
    "        graph1.remove_node(ranked_nodes.pop())\n",
    "\n",
    "        gcc=sorted(list(nx.connected_components(graph1)),key=len, reverse=True)[0]\n",
    "        gcc_size=float(len(gcc))/nnodes\n",
    "        n+=1\n",
    "        fraction_removed.append( (float(n)/nnodes, gcc_size) )\n",
    "    \n",
    "    return fraction_removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the attck sequence is the list of the airports in no particular order\n",
    "nodes=list(G.nodes())\n",
    "resilience_random=net_attack(G, nodes)\n",
    "\n",
    "nodes_betw=[]\n",
    "\n",
    "betw=nx.betweenness_centrality(G)\n",
    "for i in sorted(betw.items(), key=itemgetter(1)):\n",
    "    nodes_betw.append(i[0])\n",
    "\n",
    "resilience_betw=net_attack(G, nodes_betw)\n",
    "\n",
    "nodes_degree=[]\n",
    "\n",
    "deg=dict(G.degree())\n",
    "for i in sorted(deg.items(), key=itemgetter(1)):\n",
    "    nodes_degree.append(i[0])\n",
    "\n",
    "resilience_deg=net_attack(G, list(nodes_degree))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=[k[0] for k in resilience_random]\n",
    "y=[k[1] for k in resilience_random]\n",
    "\n",
    "x1=[k[0] for k in resilience_deg]\n",
    "y1=[k[1] for k in resilience_deg]\n",
    "\n",
    "x2=[k[0] for k in resilience_betw]\n",
    "y2=[k[1] for k in resilience_betw]\n",
    "\n",
    "plt.figure(figsize=(10,7))\n",
    "\n",
    "plt.plot(x,y, label='random attack')\n",
    "plt.plot(x1,y1, label='degree based')\n",
    "plt.plot(x2,y2, label='betw based')\n",
    "\n",
    "plt.xlabel('$f_{c}$', fontsize=22)\n",
    "plt.ylabel('$LCC$', fontsize=22)\n",
    "plt.xticks(fontsize=20)\n",
    "plt.yticks(fontsize=22)\n",
    "plt.axis([0,1,0,1])\n",
    "\n",
    "plt.legend(loc='upper right', fontsize=20)\n",
    "# y-axis is the size of the largest connected component, normalized wit hthe initial gcc's size. x-axis is treh fraction of nodses removed. note that degree or betweennes-based attacks are more effective than random attack. thus a network with a broad p_k is very weak against degree/betweeness attacks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stochastic SIR Epidemic on Static Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Parameters\n",
    "mu = 0.2           # Recovery rate \n",
    "lambd = 0.01       # Transmission rate per contact\n",
    "\n",
    "# Simulation Parameters\n",
    "nrun = 700       # Number of runs\n",
    "\n",
    "# Multi-Run Simulation\n",
    "runs = soc.network_SIR_multirun_simulation(G, nrun = nrun, lambd = lambd, mu = mu)\n",
    "\n",
    "# Set figure size \n",
    "plt.figure(figsize=(10,7))\n",
    "\n",
    "# Plot the ensemble of trajectories\n",
    "soc.plot_ensemble(runs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $\\lambda$-Sensitivity of Final Epidemic Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform lambda-sensitivity analysis of final epidemic size (normalized attack rate) \n",
    "data = soc.network_SIR_finalsize_lambda_sensitivity(G, mu = mu, rho = 0.05, # rho = initial fraction infected\n",
    "                                                    lambda_min = 0.0001, lambda_max = 1.0, \n",
    "                                                    nruns = 20)\n",
    "# Show sensitivity dataset\n",
    "data \n",
    "\n",
    "# Set figure size \n",
    "plt.figure(figsize=(10,7))\n",
    "\n",
    "# Display a boxplot with final epidemic size vs. transmission rate per edge/contact\n",
    "soc.boxplot_finalsize_lambda_sensitivity(G, mu = mu, data = data, \n",
    "                                         ymin = 0.045, ymax= 1.1,\n",
    "                                         xlim = (0.00007, 1.5) )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
